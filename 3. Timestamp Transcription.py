# -*- coding: utf-8 -*-
"""Audio File transcriber_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u6_bQzlxQG9gdWNVG9B6mx7aqEXBG1Mf
"""

# ─── Cell 1: Install/Upgrade Dependencies ───
!pip install --upgrade openai httpx

# ─── Cell 2: Clear Proxy Environment Variables (Optional) ───
import os

# Remove any stray proxy settings that can trigger the 'proxies' error
os.environ.pop("HTTP_PROXY", None)
os.environ.pop("HTTPS_PROXY", None)

# ─── Cell 3: Imports & Client Setup ───
import os
import json, re, pathlib
import pandas as pd
from tqdm import tqdm
from openai import OpenAI

# ← Set your OpenAI key here (or swap in getpass.getpass for privacy)
os.environ["OPENAI_API_KEY"] = "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"

# Initialize the HTTPX-backed client
client = OpenAI()

# ─── Cell 4: Upload Audio & Transcript ───
from google.colab import files
import pathlib, sys

print("📂  Upload ONE audio file and ONE transcript (.txt).")
uploaded = files.upload()

# Define allowed extensions
AUDIO_EXTS = (".wav", ".mp3", ".m4a", ".mp4", ".mpeg", ".mpga", ".oga", ".ogg", ".webm")
TEXT_EXTS  = (".txt",)

audio_file, transcript_file = None, None
for fn in uploaded.keys():
    low = fn.lower()
    if low.endswith(AUDIO_EXTS):
        audio_file = fn
    elif low.endswith(TEXT_EXTS):
        transcript_file = fn

if not audio_file or not transcript_file:
    sys.exit("❌  Must upload exactly ONE audio + ONE .txt transcript.")

AUDIO_PATH  = audio_file
SCRIPT_PATH = transcript_file
print(f"✅  Audio:      {AUDIO_PATH}\n✅  Transcript: {SCRIPT_PATH}")

# ─── Cell 5: Load Transcript & Split into Sentences ───
import re

with open(SCRIPT_PATH, "r", encoding="utf-8") as f:
    transcript_text = f.read().strip()

# Simple sentence split (tweak regex or switch to nltk.sent_tokenize if needed)
sentences = [
    s.strip()
    for s in re.split(r"(?<=[.!?])\s+", transcript_text)
    if s.strip()
]

print(f"ℹ️  Detected {len(sentences)} sentences.")

# ─── Cell 6: Transcribe with Whisper (Word-Level Timestamps) ───
response = client.audio.transcriptions.create(
    model                   = "whisper-1",
    file                    = open(AUDIO_PATH, "rb"),
    prompt                  = transcript_text,    # helps accuracy
    response_format         = "verbose_json",
    timestamp_granularities = ["word"]            # returns word-level times
)

words = response.words   # list of {word, start, end}
print(f"ℹ️  Received {len(words)} timestamped words.")

# ╔══════════════════════════════════════════════════════════════════════════╗
# ║  Cell 7  –  Chunked Whisper+stable-ts transcription & alignment          ║
# ║             (outputs only sentence_timestamps.csv, with “sentence” as    ║
# ║             the first column, matching your original script)             ║
# ╚══════════════════════════════════════════════════════════════════════════╝

# 1️⃣  Install dependencies (once per session)
!pip install -q stable-ts tqdm pandas

# 2️⃣  Imports
import os
import glob
import stable_whisper as whisper
from tqdm import tqdm
import pandas as pd
from datetime import timedelta
import subprocess

# 3️⃣  Automatically find a .wav file in /content
wav_files = [f for f in os.listdir(".") if f.lower().endswith(".wav")]
if not wav_files:
    raise RuntimeError("❌  No '.wav' files found in the working directory. "
                       "Please upload a .wav file (e.g. via files.upload()) and re-run.")
AUDIO_FILE = wav_files[0]
print(f"🔍  Using AUDIO_FILE = '{AUDIO_FILE}'")

# 4️⃣  Check directory contents before splitting
print("\n📂  Files in /content before splitting:")
for f in os.listdir("."):
    print("   ", f)

# 5️⃣  Prepare a folder for ~60 s chunks
CHUNK_DIR = "audio_chunks"
os.makedirs(CHUNK_DIR, exist_ok=True)

# 6️⃣  Use ffmpeg to slice the .wav into ~60 s pieces (show ffmpeg output)
segment_time = 60  # seconds per chunk (adjust if needed)
split_cmd = [
    "ffmpeg",
    "-i", AUDIO_FILE,
    "-f", "segment",
    "-segment_time", str(segment_time),
    "-c", "copy",
    os.path.join(CHUNK_DIR, "chunk_%04d.wav")
]
print(f"\n🔊  Splitting '{AUDIO_FILE}' into ~{segment_time}s chunks…")
ffmpeg_proc = subprocess.run(split_cmd)  # ffmpeg stdout/stderr will appear here
if ffmpeg_proc.returncode != 0:
    raise RuntimeError(f"❌  ffmpeg exited with code {ffmpeg_proc.returncode}. "
                       "Check the ffmpeg messages above for the cause.")

# 7️⃣  Verify that chunks exist
print("\n📂  Files in 'audio_chunks/' after splitting:")
chunk_files = sorted(glob.glob(os.path.join(CHUNK_DIR, "chunk_*.wav")))
for cf in chunk_files:
    print("   ", cf)
print(f"ℹ️  Found {len(chunk_files)} chunk file(s).")
if len(chunk_files) == 0:
    raise RuntimeError("❌  No chunk files were created! Please check that ffmpeg could read AUDIO_FILE.")

# 8️⃣  Load a smaller Whisper model (use 'small' or 'tiny' to save RAM)
model = whisper.load_model("small")  # ~1 GB RAM; switch to "tiny" (~300 MB) if needed

# 9️⃣  Prepare list to hold sentence-level timing dicts
sentence_rows = []

for idx, chunk_path in enumerate(tqdm(chunk_files, desc="Processing chunks")):
    # – Each chunk’s offset in the original file (in seconds)
    chunk_offset = idx * segment_time

    # – Transcribe this chunk with ts_num=16 (∼1 ms precision)
    result = model.transcribe(
        chunk_path,
        word_timestamps=True,
        ts_num=16
    )

    # – Gather all words from every segment into one list
    all_words_in_chunk = []
    for seg in result.segments:
        if hasattr(seg, "words") and seg.words:
            all_words_in_chunk.extend(seg.words)

    # – Build list of “sentences” (Whisper segments)
    all_sentences_in_chunk = [seg.text.strip() for seg in result.segments]

    # – For each sentence, use the corresponding word‐slice to get start/end
    w_idx = 0
    for s in all_sentences_in_chunk:
        tokens = s.split()
        token_count = len(tokens)
        if token_count == 0:
            continue

        word_slice = all_words_in_chunk[w_idx : w_idx + token_count]
        if not word_slice:
            # If not enough word‐tokens exist, skip alignment for this sentence
            w_idx += token_count
            continue

        raw_start = word_slice[0].start
        raw_end   = word_slice[-1].end

        s_start = raw_start + chunk_offset
        s_end   = raw_end   + chunk_offset

        sentence_rows.append({
            "sentence":   s,
            "start_s":    s_start,
            "end_s":      s_end,
            "duration_s": s_end - s_start,
            "start_time": str(timedelta(seconds=s_start)),
            "end_time":   str(timedelta(seconds=s_end))
        })
        w_idx += token_count

# 🔟  Convert the sentence list into a DataFrame and write to sentence_timestamps.csv
df_sentences = pd.DataFrame(sentence_rows)

# Ensure the first column is named "sentence"
df_sentences = df_sentences[["sentence", "start_s", "end_s", "duration_s", "start_time", "end_time"]]

SENT_CSV = "sentence_timestamps.csv"
df_sentences.to_csv(SENT_CSV, index=False, float_format="%.6f")
print(f"\n✅  Saved sentence-level timings → '{SENT_CSV}' ({len(df_sentences)} rows).")

# ─── Cell 8: Save & Preview ───
df = pd.DataFrame(sentence_rows)
CSV_OUT = "sentence_timestamps.csv"
df.to_csv(CSV_OUT, index=False)
print("💾  Saved →", CSV_OUT)

# Quick Pandas peek
df.head()

# ─── Cell 9: Display Interactive Table (Optional) ───
from google.colab import data_table
data_table.DataTable(df, include_index=False)

# OR, if you prefer plain Pandas:
# display(df.head())   # first 5 rows
# display(df)          # entire table